{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Денис\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Денис\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = pd.read_excel('geo_comment.xlsx')\n",
    "comment = geo['comment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare text\n",
    "\n",
    "First, we should lemmatize text, remove punctuation (leave only dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yandex Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "def lemma(word):    \n",
    "    m = Mystem()\n",
    "    return m.lemmatize(word)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better use Morphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "\n",
    "def lemma(word):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    p = morph.parse(word)[0]\n",
    "    return p.normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 of  70382\n",
      "Done 2 of  70382\n",
      "Done 3 of  70382\n",
      "Done 4 of  70382\n",
      "Done 5 of  70382\n",
      "Done 6 of  70382\n",
      "Done 7 of  70382\n",
      "Done 8 of  70382\n",
      "Done 9 of  70382\n",
      "Done 10 of  70382\n",
      "Done 11 of  70382\n",
      "Done 12 of  70382\n",
      "Done 13 of  70382\n",
      "Done 14 of  70382\n",
      "Done 15 of  70382\n",
      "Done 16 of  70382\n",
      "Done 17 of  70382\n",
      "Done 18 of  70382\n",
      "Done 19 of  70382\n",
      "Done 20 of  70382\n",
      "Done 21 of  70382\n",
      "Done 22 of  70382\n",
      "Done 23 of  70382\n",
      "Done 24 of  70382\n",
      "Done 25 of  70382\n",
      "Done 26 of  70382\n",
      "Done 27 of  70382\n",
      "Done 28 of  70382\n",
      "Done 29 of  70382\n",
      "Done 30 of  70382\n",
      "Done 31 of  70382\n",
      "Done 32 of  70382\n",
      "Done 33 of  70382\n",
      "Done 34 of  70382\n",
      "Done 35 of  70382\n",
      "Done 36 of  70382\n",
      "Done 37 of  70382\n",
      "Done 38 of  70382\n",
      "Done 39 of  70382\n",
      "Done 40 of  70382\n",
      "Done 41 of  70382\n",
      "Done 42 of  70382\n",
      "Done 43 of  70382\n",
      "Done 44 of  70382\n",
      "Done 45 of  70382\n",
      "Done 46 of  70382\n",
      "Done 47 of  70382\n",
      "Done 48 of  70382\n",
      "Done 49 of  70382\n",
      "Done 50 of  70382\n",
      "Done 51 of  70382\n",
      "Done 52 of  70382\n",
      "Done 53 of  70382\n",
      "Done 54 of  70382\n",
      "Done 55 of  70382\n",
      "Done 56 of  70382\n",
      "Done 57 of  70382\n",
      "Done 58 of  70382\n",
      "Done 59 of  70382\n",
      "Done 60 of  70382\n",
      "Done 61 of  70382\n",
      "Done 62 of  70382\n",
      "Done 63 of  70382\n",
      "Done 64 of  70382\n",
      "Done 65 of  70382\n",
      "Done 66 of  70382\n",
      "Done 67 of  70382\n",
      "Done 68 of  70382\n",
      "Done 69 of  70382\n",
      "Done 70 of  70382\n",
      "Done 71 of  70382\n",
      "Done 72 of  70382\n",
      "Done 73 of  70382\n",
      "Done 74 of  70382\n",
      "Done 75 of  70382\n",
      "Done 76 of  70382\n",
      "Done 77 of  70382\n",
      "Done 78 of  70382\n",
      "Done 79 of  70382\n",
      "Done 80 of  70382\n",
      "Done 81 of  70382\n",
      "Done 82 of  70382\n",
      "Done 83 of  70382\n",
      "Done 84 of  70382\n",
      "Done 85 of  70382\n",
      "Done 86 of  70382\n",
      "Done 87 of  70382\n",
      "Done 88 of  70382\n",
      "Done 89 of  70382\n",
      "Done 90 of  70382\n",
      "Done 91 of  70382\n",
      "Done 92 of  70382\n",
      "Done 93 of  70382\n",
      "Done 94 of  70382\n",
      "Done 95 of  70382\n",
      "Done 96 of  70382\n",
      "Done 97 of  70382\n",
      "Done 98 of  70382\n",
      "Done 99 of  70382\n",
      "Done 100 of  70382\n",
      "Done 101 of  70382\n",
      "Done 102 of  70382\n",
      "Done 103 of  70382\n",
      "Done 104 of  70382\n",
      "Done 105 of  70382\n",
      "Done 106 of  70382\n",
      "Done 107 of  70382\n",
      "Done 108 of  70382\n",
      "Done 109 of  70382\n",
      "Done 110 of  70382\n",
      "Done 111 of  70382\n",
      "Done 112 of  70382\n",
      "Done 113 of  70382\n",
      "Done 114 of  70382\n",
      "Done 115 of  70382\n",
      "Done 116 of  70382\n",
      "Done 117 of  70382\n",
      "Done 118 of  70382\n",
      "Done 119 of  70382\n",
      "Done 120 of  70382\n",
      "Done 121 of  70382\n",
      "Done 122 of  70382\n",
      "Done 123 of  70382\n",
      "Done 124 of  70382\n",
      "Done 125 of  70382\n",
      "Done 126 of  70382\n",
      "Done 127 of  70382\n",
      "Done 128 of  70382\n",
      "Done 129 of  70382\n",
      "Done 130 of  70382\n",
      "Done 131 of  70382\n",
      "Done 132 of  70382\n",
      "Done 133 of  70382\n",
      "Done 134 of  70382\n",
      "Done 135 of  70382\n",
      "Done 136 of  70382\n",
      "Done 137 of  70382\n",
      "Done 138 of  70382\n",
      "Done 139 of  70382\n",
      "Done 140 of  70382\n",
      "Done 141 of  70382\n",
      "Done 142 of  70382\n",
      "Done 143 of  70382\n",
      "Done 144 of  70382\n",
      "Done 145 of  70382\n",
      "Done 146 of  70382\n",
      "Done 147 of  70382\n",
      "Done 148 of  70382\n",
      "Done 149 of  70382\n",
      "Done 150 of  70382\n",
      "Done 151 of  70382\n",
      "Done 152 of  70382\n",
      "Done 153 of  70382\n",
      "Done 154 of  70382\n",
      "Done 155 of  70382\n",
      "Done 156 of  70382\n",
      "Done 157 of  70382\n",
      "Done 158 of  70382\n",
      "Done 159 of  70382\n",
      "Done 160 of  70382\n",
      "Done 161 of  70382\n",
      "Done 162 of  70382\n",
      "Done 163 of  70382\n",
      "Done 164 of  70382\n",
      "Done 165 of  70382\n",
      "Done 166 of  70382\n",
      "Done 167 of  70382\n",
      "Done 168 of  70382\n",
      "Done 169 of  70382\n",
      "Done 170 of  70382\n",
      "Done 171 of  70382\n",
      "Done 172 of  70382\n",
      "Done 173 of  70382\n",
      "Done 174 of  70382\n",
      "Done 175 of  70382\n",
      "Done 176 of  70382\n",
      "Done 177 of  70382\n",
      "Done 178 of  70382\n",
      "Done 179 of  70382\n",
      "Done 180 of  70382\n",
      "Done 181 of  70382\n",
      "Done 182 of  70382\n",
      "Done 183 of  70382\n",
      "Done 184 of  70382\n",
      "Done 185 of  70382\n",
      "Done 186 of  70382\n",
      "Done 187 of  70382\n",
      "Done 188 of  70382\n",
      "Done 189 of  70382\n",
      "Done 190 of  70382\n",
      "Done 191 of  70382\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# tokenize to words\n",
    "# lemmatize them\n",
    "# and then rebuild the text\n",
    "def clean_text(text):\n",
    "    parts = nltk.word_tokenize(text)\n",
    "    \n",
    "    restrict_parts = [':', ',', ';']\n",
    "    \n",
    "    # allow punctuation\n",
    "    punct = ['.', '-']\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    for part in parts:\n",
    "        if part not in restrict_parts:\n",
    "            if part in punct:\n",
    "                word = part\n",
    "            # lemmatize\n",
    "            else:\n",
    "                word = ' ' + lemma(part) \n",
    "            result += word\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    \n",
    "#comment['comment'] = comment['comment'].apply(clean_text)\n",
    "new_comment = []\n",
    "i = 0\n",
    "for c in comment:\n",
    "    new_comment.append(clean_text(c))\n",
    "    i += 1\n",
    "    print(f'Done {i} of ', len(comment))\n",
    "    \n",
    "f = open('normalized_comments.pkl', 'rb')\n",
    "pickle.dump(new_comment, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load sentences tokenizer\n",
    "sent_token = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "\n",
    "# each comment to sentences\n",
    "sentences = comment.apply(sent_token.tokenize)\n",
    "\n",
    "# concencate in one list\n",
    "sentences = itertools.chain.from_iterable(sentences.tolist())\n",
    "\n",
    "# tokenize to words as gensim accept sentences in list of words format\n",
    "sent_list = []\n",
    "for sent in sentences:\n",
    "    sent_list += [nltk.word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create phrases model     \n",
    "phrases = Phrases(sent_list, min_count=10, threshold=10)\n",
    "\n",
    "# Export the trained model = use less RAM, faster processing. Model updates no longer possible.\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "# save model\n",
    "bigram.save(\"bigram_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def chunks(text, model):\n",
    "    result = ''\n",
    "    \n",
    "    sentences = sent_token.tokenize(text)\n",
    "    for sent in sentences:\n",
    "        for part in model[nltk.word_tokenize(sent)]:\n",
    "            # rebuild text\n",
    "            if part != '.':\n",
    "                result += part\n",
    "            else:\n",
    "                result += ' ' + part\n",
    "        \n",
    "\n",
    "bigram = Phraser.load(\"bigram_model.pkl\")\n",
    "\n",
    "chunked_comments = [chunks(x, bigram) for x in new_comment]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "\n",
    "# we must remove stop words\n",
    "sw = stopwords.words('russian')\n",
    "with open('comment_stop_words.txt', encoding='utf-8') as f:\n",
    "    our_stop_words = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "sw = sw + our_stop_words\n",
    "\n",
    "# We will remove digits\n",
    "word_pattern = r'[a-zA-Zа-яА-Я]+'\n",
    "\n",
    "cv = CountVectorizer(stop_words=sw, token_pattern=word_pattern)\n",
    "data_cv = cv.fit_transform(chunked_comments)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "#del cv\n",
    "#del data_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save document term matrix to file\n",
    "Need a lot of memory for saving it as Dataframe. So we save it as sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dtm.pkl', 'wb')\n",
    "pickle.dump(data_cv, f)\n",
    "f.close()\n",
    "\n",
    "# save countvectorizer\n",
    "f = open('cv.pkl', 'wb')\n",
    "pickle.dump(cv, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load document term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dtm.pkl', 'wb')\n",
    "data_dtm = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Text count: ', data_dtm.shape[0], ' Word count: ', data_dtm.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "words = {}\n",
    "\n",
    "for word in data_dtm.columns:\n",
    "    # first lemmatize colum\n",
    "    w = lemma(word)\n",
    "    \n",
    "    if w in words:\n",
    "        words[w] += data_dtm[word].sum()\n",
    "    else:\n",
    "        words[w] = data_dtm[word].sum() \n",
    "\n",
    "# save to DF, sort\n",
    "w = pd.DataFrame.from_dict(words, orient='index')\n",
    "w.sort_values(by=0,ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "w.to_csv('word_stats.csv', encoding='utf-8')\n",
    "\n",
    "f = open('words.pkl', 'wb')\n",
    "pickle.dump(words, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('words.pkl', 'rb')\n",
    "words = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)\n",
    "\n",
    "wc = wc.generate_from_frequencies(words)\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at freq of words\n",
    "First of all, i've added some words for our stop dictionary.\n",
    "\n",
    "As we see, most the messages are related to land use and construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "* LDA\n",
    "* LSI (Latent Semantic Indexing)\n",
    "* NMF Non-Negative Matrix Factorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "As it in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.matutils.Sparse2Corpus at 0x1777777ca48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# we have to transpose\n",
    "# convert from df to sparse matrix\n",
    "sparse_counts = scipy.sparse.csr_matrix(data_dtm.transpose())\n",
    "\n",
    "# sparse matrix to corpus\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sparse2corpus.pkl', 'wb')\n",
    "pickle.dump(corpus, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.110*\"москвы\" + 0.073*\"города\" + 0.046*\"землепользования\" + 0.034*\"округа\" + 0.030*\"административном\" + 0.029*\"северо\" + 0.027*\"западном\" + 0.026*\"территории\" + 0.025*\"правил\" + 0.021*\"функциональной\"'),\n",
       " (1,\n",
       "  '0.034*\"против\" + 0.030*\"категорически\" + 0.029*\"территории\" + 0.020*\"героев\" + 0.018*\"проект\" + 0.018*\"придомовой\" + 0.017*\"д\" + 0.016*\"застройки\" + 0.016*\"внутри\" + 0.013*\"строительства\"'),\n",
       " (2,\n",
       "  '0.074*\"территории\" + 0.056*\"шоссе\" + 0.036*\"требуем\" + 0.030*\"серп\" + 0.030*\"молот\" + 0.030*\"энтузиастов\" + 0.021*\"комплекса\" + 0.019*\"процентом\" + 0.019*\"отвести\" + 0.019*\"вал\"'),\n",
       " (3,\n",
       "  '0.031*\"панфиловцев\" + 0.027*\"дома\" + 0.021*\"проекта\" + 0.021*\"жителей\" + 0.021*\"территории\" + 0.021*\"свободы\" + 0.020*\"адресу\" + 0.018*\"д\" + 0.016*\"жилого\" + 0.015*\"проект\"'),\n",
       " (4,\n",
       "  '0.017*\"москвы\" + 0.016*\"использования\" + 0.015*\"земельных\" + 0.015*\"участков\" + 0.014*\"кодекса\" + 0.014*\"градостроительного\" + 0.013*\"города\" + 0.013*\"пзз\" + 0.012*\"москве\" + 0.011*\"соответствии\"'),\n",
       " (5,\n",
       "  '0.050*\"застройки\" + 0.049*\"проект\" + 0.041*\"правил\" + 0.041*\"землепользования\" + 0.031*\"москвы\" + 0.028*\"публичных\" + 0.028*\"слушаний\" + 0.021*\"слушания\" + 0.021*\"публичные\" + 0.020*\"проекту\"'),\n",
       " (6,\n",
       "  '0.070*\"митино\" + 0.021*\"района\" + 0.015*\"близлежащих\" + 0.015*\"настоящее\" + 0.015*\"время\" + 0.015*\"жителей\" + 0.014*\"пятницкое\" + 0.014*\"москвы\" + 0.013*\"жители\" + 0.013*\"которые\"'),\n",
       " (7,\n",
       "  '0.033*\"гпзу\" + 0.031*\"муниципального\" + 0.027*\"участка\" + 0.025*\"застройки\" + 0.021*\"регламент\" + 0.017*\"адресу\" + 0.017*\"решение\" + 0.015*\"проведения\" + 0.015*\"назначением\" + 0.015*\"участок\"'),\n",
       " (8,\n",
       "  '0.028*\"метро\" + 0.023*\"требуем\" + 0.023*\"строительство\" + 0.023*\"тпу\" + 0.017*\"пзз\" + 0.015*\"станции\" + 0.014*\"жители\" + 0.012*\"противоречит\" + 0.010*\"провести\" + 0.009*\"против\"'),\n",
       " (9,\n",
       "  '0.019*\"объектов\" + 0.018*\"зоны\" + 0.016*\"книга\" + 0.016*\"застройки\" + 0.015*\"м\" + 0.014*\"также\" + 0.011*\"культурного\" + 0.011*\"площадь\" + 0.010*\"территории\" + 0.010*\"охраны\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "\n",
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\n",
    "lda.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
